{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\88015\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-24 16:04:42,522 : INFO : running C:\\Users\\88015\\Anaconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\88015\\AppData\\Roaming\\jupyter\\runtime\\kernel-053937b1-763b-44a2-aca0-643ff5068728.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-24 16:04:42,657 : INFO : loading Doc2Vec object from ./pretrained.d2v\n",
      "2018-07-24 16:04:43,969 : INFO : loading vocabulary recursively from ./pretrained.d2v.vocabulary.* with mmap=None\n",
      "2018-07-24 16:04:43,970 : INFO : loading trainables recursively from ./pretrained.d2v.trainables.* with mmap=None\n",
      "2018-07-24 16:04:43,970 : INFO : loading wv recursively from ./pretrained.d2v.wv.* with mmap=None\n",
      "2018-07-24 16:04:43,971 : INFO : loading docvecs recursively from ./pretrained.d2v.docvecs.* with mmap=None\n",
      "2018-07-24 16:04:43,972 : INFO : loaded ./pretrained.d2v\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec.load('./pretrained.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = np.zeros((8000, 100))\n",
    "train_labels = np.zeros((8000,2))\n",
    "\n",
    "for i in range(4000):\n",
    "    train_neg = \"TRAIN_NEG_\"+str(i)\n",
    "    train_pos= \"TRAIN_POS_\"+str(i)\n",
    "    prefix_train_pos = model.docvecs[train_pos] \n",
    "    prefix_train_neg = model.docvecs[train_neg] \n",
    "    train_arrays[i] = prefix_train_pos\n",
    "    train_arrays[4000 + i] = prefix_train_neg\n",
    "    train_labels[i][0] = 1\n",
    "    train_labels[4000 + i][1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = np.zeros((400, 100))\n",
    "test_labels = np.zeros((400,2))\n",
    "\n",
    "for i in range(200):\n",
    "    test_neg = \"TEST_NEG_\"+str(i)\n",
    "    test_pos= \"TEST_POS_\"+str(i)\n",
    "    prefix_test_pos = model.docvecs[test_pos] \n",
    "    prefix_test_neg = model.docvecs[test_neg] \n",
    "    test_arrays[i] = prefix_test_pos\n",
    "    test_arrays[200 + i] = prefix_test_neg\n",
    "    test_labels[i][0] = 1\n",
    "    test_labels[200 + i][1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model\n",
    "\n",
    "#3 hidden layers\n",
    "#nodes for each hidden layer\n",
    "\n",
    "epochs = 6\n",
    "n_classes = 2\n",
    "batch_size = 128\n",
    "chunk_size = 5\n",
    "n_chunk = 20\n",
    "rnn_size = 128\n",
    "\n",
    "#height x width\n",
    "x = tf.placeholder('float', [None, n_chunk, chunk_size])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def recurrent_neural_network(data):\n",
    " \n",
    "    \n",
    "    \n",
    "    layer = {'weights':tf.Variable(tf.random_normal([rnn_size, n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, chunk_size])\n",
    "    x = tf.split(x, n_chunk, 0)\n",
    "    \n",
    "    lstm_cell= rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype = tf.float32) \n",
    "    \n",
    "\n",
    "    \n",
    "    output = tf.matmul(outputs[-1], layer['weights']) + layer['biases']\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nural_network(x):\n",
    "    prediction = recurrent_neural_network(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        _sentinel=None,\n",
    "        labels= y,\n",
    "        logits= prediction,\n",
    "        dim=-1))\n",
    "    #learning_rate = 0.001 default\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    global sess\n",
    "    if(True):\n",
    "#     with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            k = 0\n",
    "            \n",
    "            while k< len(train_arrays):\n",
    "                start = k\n",
    "                end = k+ batch_size\n",
    "                batch_x = np.array(train_arrays[start:end])\n",
    "                batch_y = np.array(train_labels[start:end])\n",
    "                batch_x = batch_x.reshape((batch_size, n_chunk, chunk_size))\n",
    "                \n",
    "                i, c = sess.run([optimizer, cost], feed_dict = {x:batch_x, y:batch_y})\n",
    "                epoch_loss += c\n",
    "                k += batch_size\n",
    "                \n",
    "            #print('Epoch ', epoch, 'completed out of ', epochs, 'loss', epoch_loss, 'acc:',i )\n",
    "            \n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x:test_arrays.reshape((-1, n_chunk, chunk_size)),\n",
    "                                          y: test_labels})*100, '%')\n",
    "        \n",
    "                \n",
    "            \n",
    "        \n",
    "#train_nural_network(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'x' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ee0eddcbe112>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_nural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-96b17717f89f>\u001b[0m in \u001b[0;36mtrain_nural_network\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_nural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecurrent_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n\u001b[0;32m      4\u001b[0m         \u001b[0m_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-09f6ea73118e>\u001b[0m in \u001b[0;36mrecurrent_neural_network\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     24\u001b[0m              'biases':tf.Variable(tf.random_normal([n_classes]))}\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train_nural_network(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
